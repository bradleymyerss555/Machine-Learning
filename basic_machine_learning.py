# -*- coding: utf-8 -*-
"""Basic Machine Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jEiuKOQJDQxUBrBUH7H5TPFGJ9xoE3fX

# Basic Machine Learning

Each model is split into training and test sets (80 20). The following machine learning algorithms are used to develop a model to predict the sentiment of a sample of tweets: Random Forest, XGboost, Decision Tree, Logistic Regression, Naive Bayes, and K Nearest Neighbors. Each model is fit to the training set and evaluated on it's ability to predict the sentiment of tweets in the test set. The accuracy is displayed for each model. The most accurate model is the Logistic Regression Algorithm, with 94% accuracy.
"""

# Setup

import numpy as np
import pandas as pd

import scipy
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score

# Data Preparation
from google.colab import drive
drive.mount('/content/drive', force_remount = True)

# File Path
file_path = '/content/drive/My Drive/MSBA/Spring 2024/BMSO758Q/Assignments/Basic Machine Learning/Tweets_sentiment.csv'

# Loading Data
tweets = pd.read_csv(file_path)
tweets

# Clean text data
tweets.dropna(subset=['clean_text'], inplace= True)
tweets.dropna(subset=['category'], inplace= True)

# Define variables
x = tweets['clean_text']
y = tweets['category']

# Process text data
vectorize = CountVectorizer()
x = vectorize.fit_transform(x)

# Training and Testing Splits
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state = 1234)

# Random Forest Model
rf_model = RandomForestClassifier(n_estimators=10, random_state=1234)
rf_model.fit(x_train, y_train)

# Evaluate RF Model
y_pred = rf_model.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(y_test, y_pred))

# XGboost Model

# Adjust label
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

xgboost_model = XGBClassifier(objective='multi:softmax', num_class=3)
xgboost_model.fit(x_train, y_train)

# Evaluate XGboost Model
y_pred = xgboost_model.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Re-define variables
x = tweets['clean_text']
y = tweets['category']

# Process text data
vectorize = CountVectorizer()
x = vectorize.fit_transform(x)

# Decision Tree Model
dt = DecisionTreeClassifier()
dt.fit(x_train, y_train)

# Evaluate Decision Tree Model
y_pred = dt.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Logistic Regression Model
log = LogisticRegression(max_iter=1000)
log.fit(x_train, y_train)

# Evaluate Logistic Regression Model
y_pred = log.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Naive Bayes Model
NB = MultinomialNB()
NB.fit(x_train, y_train)

# Evaluating Naive Bayes Model
y_pred = NB.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(y_test, y_pred))

# KNN Model
knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(x_train, y_train)

# Evaluate KNN Model
y_pred = knn.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(y_test, y_pred))